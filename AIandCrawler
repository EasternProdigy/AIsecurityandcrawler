import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import requests
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import colorama

            ######################
            #Function Definitions#
            ######################

def makeTokens(f):
    tkns_BySlash = str(f.encode('utf-8')).split('/') # make tokens after splitting by slash
    total_Tokens = []

    for i in tkns_BySlash:
        tokens = str(i).split('-') # make tokens after splitting by dash
        tkns_ByDot = []

    for j in range(0,len(tokens)):
        temp_Tokens = str(tokens[j]).split('.') # make tokens after splitting by dot
        tkns_ByDot = tkns_ByDot + temp_Tokens
        total_Tokens = total_Tokens + tokens + tkns_ByDot
        total_Tokens = list(set(total_Tokens))  #remove redundant tokens

    if 'com' in total_Tokens:
        total_Tokens.remove('com') # removing .com since it occurs a lot of times and it should not be included in our features

    return total_Tokens

def is_valid(url):
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)

def get_all_website_links(url):

    urls = []
    domain_name = urlparse(url).netloc
    soup = BeautifulSoup(requests.get(url).content, "html.parser")
    
    for a_tag in soup.findAll("a"):
        href = a_tag.attrs.get("href")
        if href == "" or href is None:
            continue
        href = urljoin(url, href)
        parsed_href = urlparse(href)
        href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path
        if not is_valid(href):
            continue
        if href in urls:
            continue
        urls.append(str(href))
    return urls
        


            #############################
            #Training and Initialization#
            #############################


def main():
    colorama.init()
    GREEN = colorama.Fore.GREEN
    RED = colorama.Fore.RED
    YELLOW = colorama.Fore.YELLOW
    RESET = colorama.Fore.RESET
    website = str(input("Input Website To Scan: "))
    print(f"{YELLOW} CRAWLING AND SCANNING... {RESET}")
    urls_data = pd.read_csv(r"C:\Users\William Mezitis\Downloads\urldata(1).csv")
    urls_data.head()

    url_list = urls_data["url"]
    y = urls_data["label"]
    vectorizer = TfidfVectorizer(tokenizer=makeTokens)
    X = vectorizer.fit_transform(url_list)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    logit = LogisticRegression(max_iter=1000)
    logit.fit(X_train, y_train)

                #####################
                #Setting Up URL Code#
                #####################
    urls = get_all_website_links(website)
    results = vectorizer.transform(urls)
    final = logit.predict(results)
    for x in range(len(urls)):
        if (final[x] == "bad"):
            print(f"{RED} [!] Potentially Harmful: {urls[x]} {RESET}")
        else:
            print(f"{GREEN} [*] Safe: {urls[x]} {RESET}")

main()
